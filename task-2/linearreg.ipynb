{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ammar Al-saeed Mohammed , Sec 2 , BN 1\n",
    "## This Code works on any shape of data (both multivariate and univariat)\n",
    "## simply change the name of the file in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "datamatrix = []\n",
    "labelslist =[]\n",
    "with open ('univariateData.dat') as data_file :\n",
    "    for line in data_file :\n",
    "        line_list = line.strip ('\\n').split (',')\n",
    "        dataline= []\n",
    "        for i in range(len(line_list)-1):\n",
    "            dataline.append(float(line_list[i]))\n",
    "        datamatrix.append(list(dataline))\n",
    "        labelslist.append(line_list[-1])\n",
    "features = np.asarray(datamatrix,dtype=\"float\")\n",
    "labels = np.asarray(labelslist,dtype=\"float\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_test, y, Y_test = train_test_split(features, labels, test_size=0.20, random_state=1)\n",
    "X_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "theta = np.ones(X.shape[1])\n",
    "theta = np.transpose(theta)\n",
    "print(X)\n",
    "print(type(theta[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]\n",
    "\n",
    "def hypothesis(theta,X):\n",
    "    # x_transposed = np.transpose(X)\n",
    "    hypo = X.dot(theta) \n",
    "    return  hypo\n",
    "\n",
    "def compute_cost(theta,X,y):\n",
    "\n",
    "    cost = 1/(2*m) * sum(np.square(hypothesis(theta,X)-y))\n",
    "    return cost\n",
    "def derivative(paramnum,theta,X,y):\n",
    "    \n",
    "    hypo = hypothesis(theta,X)\n",
    "    derivative =(hypo-y) * X[:,paramnum]\n",
    "    sum_deravites = sum(derivative) * 1/m\n",
    "    return sum_deravites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to test derivative \n",
    "# testdev = derivative(0,theta,X,y)\n",
    "# hypo = hypothesis(theta,X)\n",
    "# print(testdev - (hypo-y))\n",
    "def gradient_descent(theta,X,y,lr):\n",
    "    costs = [0] #arbitrary numbers to ensure the loop starts\n",
    "    cost = compute_cost(theta,X,y)\n",
    "    costs.append(cost)\n",
    "    #print(\"costs: \", costs)\n",
    "    temp_thetas = np.ones(len(theta))\n",
    "    iterations = 0\n",
    "    while(abs(costs[-1]-costs[-2])>0.00001 and iterations<1500000):\n",
    "        for paramnum in range(len(theta)):\n",
    "             temp_thetas[paramnum] = theta[paramnum] - lr * derivative(paramnum,theta,X,y)\n",
    "             \n",
    "        for paramnum in range(len(theta)):\n",
    "            theta[paramnum] = temp_thetas[paramnum]\n",
    "        iterations = iterations+1\n",
    "        cost = compute_cost(theta,X,y)\n",
    "        costs.append(cost)\n",
    "    print(\"lastcosts: \", costs[-1], costs[-2])\n",
    "    print(iterations,\"iterations reached\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advised learning rate for multivariate so it don't diverge is 0.00000041 or lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lastcosts:  6.947110956955219 6.947120956321\n",
      "44067 iterations reached\n",
      "1.758291960174026\n"
     ]
    }
   ],
   "source": [
    "def fit(theta,X,y,lr=0.01):\n",
    "    theta = gradient_descent(theta,X,y,lr)\n",
    "    return theta\n",
    "theta = fit(theta,X,y,lr=0.00000041)\n",
    "\n",
    "def predict(theta,X_test):\n",
    "   predicted_values =  hypothesis(theta,X_test)\n",
    "   return predicted_values\n",
    "\n",
    "def evaluate_performance(theta,X_test,Y_test):\n",
    "    performance = compute_cost(theta,X_test,Y_test)\n",
    "    return performance\n",
    "\n",
    "performance = evaluate_performance(theta,X_test,Y_test)\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what's the difference between linear regression and LASSO regression? \n",
    " LASSO regression has the same cost function of linear regression + summation of the absolute of the weights times a factor (a hyper parameter like the learning rate for instance). \n",
    " ## what will happen if we used LASSO regression instead?\n",
    " since the cost function now increases when the absolute of any weight increases, the model will try to lower every weight as much as possible while maintaining the first term of the cost function low as well, to result in the lowest cost possible. Essentially, what this means is LASSO regression introduces some form of feature selection, i.e. some features will have very low weights, thus have a negligible effect."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "588d49391b6d3799a162df0222124dcb982f2f3fb55a5b5f4bcdbe6ca376e2a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
